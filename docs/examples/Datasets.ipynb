{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with MillionTrees datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TreePoints', 'TreeBoxes', 'TreePolygons']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'examples':\n",
    "    sys.path.append(\"../\")\n",
    "    \n",
    "import milliontrees\n",
    "from torchvision import transforms\n",
    "\n",
    "# List available datasets\n",
    "print(milliontrees.benchmark_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general workflow is to \n",
    "1. Select and optionally download a dataset\n",
    "2. Load the train and test splits from the dataset\n",
    "3. Create the dataloader, with optional additional transforms, for how to preprocess images, and optionally augment, input images and metadata\n",
    "4. Use these dataloaders to train models in native pytorch or pytorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and optionally download a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/ewhite/b.weinstein/miniconda3/envs/MillionTrees/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/blue/ewhite/b.weinstein/miniconda3/envs/MillionTrees/lib/python3.10/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Load the box dataset\n",
    "from milliontrees import get_dataset\n",
    "dataset = get_dataset(\"TreeBoxes\", root_dir=\"/orange/ewhite/DeepForest/MillionTrees/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test splits from the dataset\n",
    "\n",
    "Datasets are split into groups of images based on task. For example, 'train' versus 'test' or 'zero_shot_train' and 'zero_shot_test'. The get_subset function has a 'frac' argument to subsample the data during rapid testing and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata length: 2\n",
      "Image shape: torch.Size([3, 448, 448]), Image type: <class 'torch.Tensor'>\n",
      "Targets keys: dict_keys(['y', 'labels']), Label type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from milliontrees.common.data_loaders import get_train_loader\n",
    "\n",
    "train_dataset = dataset.get_subset(\"train\")\n",
    "\n",
    "# View the first image in the dataset\n",
    "metadata, image, targets = train_dataset[0]\n",
    "print(f\"Metadata length: {len(metadata)}\")\n",
    "print(f\"Image shape: {image.shape}, Image type: {type(image)}\")\n",
    "print(f\"Targets keys: {targets.keys()}, Label type: {type(targets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You are loading the entire dataset. Consider using dataset.get_subset('train') for a portion of the dataset if intended.\n",
      "Targets is a list of dictionaries with the following keys:  dict_keys(['y', 'labels'])\n",
      "Image shape: torch.Size([2, 3, 448, 448]), Image type: <class 'torch.Tensor'>\n",
      "Annotation shape of the first image: torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_train_loader(\"standard\", train_dataset, batch_size=2)\n",
    "\n",
    "# Show one batch of the loader\n",
    "for metadata, image, targets in train_loader:\n",
    "    print(\"Targets is a list of dictionaries with the following keys: \", targets[0].keys())\n",
    "    print(f\"Image shape: {image.shape}, Image type: {type(image)}\")\n",
    "    print(f\"Annotation shape of the first image: {targets[0]['y'].shape}\")\n",
    "    break  # Just show the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/blue/ewhite/b.weinstein/miniconda3/envs/MillionTrees/lib/python3.10/site-packages/albumentations/core/bbox_utils.py:468: RuntimeWarning: invalid value encountered in divide\n",
      "  & (clipped_box_areas / denormalized_box_areas >= min_visibility - EPSILON)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     all_y_pred\u001b[38;5;241m.\u001b[39mappend(targets)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_y_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_y_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/datasets/TreeBoxes.py:152\u001b[0m, in \u001b[0;36mTreeBoxesDataset.eval\u001b[0;34m(self, y_pred, y_true, metadata)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_pred, y_true, metadata):\n\u001b[1;32m    147\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    The main evaluation metric, detection_acc_avg_dom,\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    measures the simple average of the detection accuracies\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    of each domain.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     results, results_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstandard_group_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eval_grouper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     detection_accs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/datasets/milliontrees_dataset.py:437\u001b[0m, in \u001b[0;36mMillionTreesDataset.standard_group_eval\u001b[0;34m(metric, grouper, y_pred, y_true, metadata, aggregate)\u001b[0m\n\u001b[1;32m    435\u001b[0m results, results_str \u001b[38;5;241m=\u001b[39m {}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aggregate:\n\u001b[0;32m--> 437\u001b[0m     results\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    438\u001b[0m     results_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[metric\u001b[38;5;241m.\u001b[39magg_metric_field]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m g \u001b[38;5;241m=\u001b[39m grouper\u001b[38;5;241m.\u001b[39mmetadata_to_group(metadata)\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/common/metrics/metric.py:88\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, y_pred, y_true, return_dict)\u001b[0m\n\u001b[1;32m     86\u001b[0m         agg_metric \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     agg_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[1;32m     90\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_metric_field: agg_metric\u001b[38;5;241m.\u001b[39mitem()}\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/common/metrics/metric.py:178\u001b[0m, in \u001b[0;36mElementwiseMetric._compute\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute\u001b[39m(\u001b[38;5;28mself\u001b[39m, y_pred, y_true):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper function for computing the metric.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m        - avg_metric (0-dim tensor): average of element-wise metrics\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     element_wise_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_element_wise\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     avg_metric \u001b[38;5;241m=\u001b[39m element_wise_metrics\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_metric\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/common/metrics/all_metrics.py:390\u001b[0m, in \u001b[0;36mDetectionAccuracy._compute_element_wise\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target, batch_boxes_predictions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y_true, y_pred):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# concat all boxes and scores\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_results[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometry_name] \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m batch_boxes_predictions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m     pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m batch_boxes_predictions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    391\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes[pred_scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_threshold]\n\u001b[1;32m    392\u001b[0m     src_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_results[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometry_name] \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m target], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/MillionTrees/milliontrees/common/metrics/all_metrics.py:390\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target, batch_boxes_predictions \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(y_true, y_pred):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# concat all boxes and scores\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_results[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometry_name] \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m batch_boxes_predictions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 390\u001b[0m     pred_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mimage_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m batch_boxes_predictions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    391\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes[pred_scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_threshold]\n\u001b[1;32m    392\u001b[0m     src_boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_results[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeometry_name] \u001b[38;5;28;01mfor\u001b[39;00m image_results \u001b[38;5;129;01min\u001b[39;00m target], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'score'"
     ]
    }
   ],
   "source": [
    "from milliontrees.common.data_loaders import get_eval_loader\n",
    "import torch\n",
    "\n",
    "test_dataset = dataset.get_subset(\"train\")\n",
    "test_loader = get_eval_loader(\"standard\", test_dataset, batch_size=16)\n",
    "\n",
    "# Show one batch of the loader\n",
    "all_y_pred = []\n",
    "all_y_true = []\n",
    "all_metadata = []\n",
    "\n",
    "def predictor(images):\n",
    "    return [{'y': torch.tensor([[30, 70, 35, 75]]),\n",
    "             'label': torch.tensor([0]),\n",
    "             'score': torch.tensor([0.54])} for _ in range(images.shape[0])]\n",
    "\n",
    "# For the sake of this example, we will make up some predictions to show format\n",
    "for metadata, images, targets in test_loader:\n",
    "    all_metadata.append(metadata)\n",
    "    all_y_true.append(targets)\n",
    "    all_y_pred.append(predictor(images))\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "dataset.eval(all_y_pred, all_y_true, all_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MillionTrees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
